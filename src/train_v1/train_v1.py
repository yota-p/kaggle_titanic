import sys
import time
from pathlib import Path
import pandas as pd
import numpy as np
import xgboost as xgb
import lightgbm as lgb
import catboost
import mlflow
import hydra
import pickle
import shutil
import pprint
import warnings
from typing import List, Tuple, Any
from omegaconf.dictconfig import DictConfig
from sklearn.metrics import roc_auc_score
from src.train_v1.util.get_environment import get_exec_env, get_datadir, is_gpu, is_ipykernel
from src.train_v1.util.fast_fillna import fast_fillna
from src.train_v1.models.PurgedGroupTimeSeriesSplit import PurgedGroupTimeSeriesSplit
from src.train_v1.util.calc_utility_score import utility_score_pd_scaled
from src.train_v1.util.calc_cross_feature import calc_cross_feature
warnings.filterwarnings("ignore")


def create_janeapi() -> Tuple[Any, Any]:
    DATA_DIR = get_datadir()
    if get_exec_env() not in ['kaggle-Interactive', 'kaggle-Batch']:
        sys.path.append(f'{DATA_DIR}/raw')
    import janestreet
    env = janestreet.make_env()  # initialize the environment
    iter_test = env.iter_test()  # an iterator which loops over the test set
    return env, iter_test


def predict(models: List[Any], feature_engineering: DictConfig, target: str, OUT_DIR: str) -> None:
    '''
    Note: Be aware of the performance in the 'for' loop!
    Prediction API generates 1*130 DataFrame per iteration.
    Which means you need to process every record separately. (no vectorization)
    If the performance in 'for' loop is poor, it'll result in submission timeout.
    '''
    env, iter_test = create_janeapi()
    feat_cols = [f'feature_{i}' for i in range(130)]
    print('Start predicting')
    time_start = time.time()
    if feature_engineering.method_fillna == 'forward':
        tmp = np.zeros(len(feat_cols))  # this np.ndarray will contain last seen values for features

    for (test_df, pred_df) in iter_test:  # iter_test generates test_df(1,130)
        if test_df['weight'].item() > 0:  # cut-off by weight
            # For forward fillna, using high-performance logic by Yirun Zhang
            if feature_engineering.method_fillna == 'forward':
                x_tt = test_df.loc[:, feat_cols].values  # this is (1,130) ndarray([[values...]])
                x_tt[0, :] = fast_fillna(x_tt[0, :], tmp)  # use values in tmp to replace nan
                tmp = x_tt[0, :]  # save last seen values to tmp
            elif feature_engineering.method_fillna == '-999':
                X_test = test_df.loc[:, feat_cols]
                X_test.fillna(-999)
                x_tt = X_test.values
            else:
                raise ValueError(f'Invalid feature_engineering.method_fillna: {feature_engineering.method_fillna}')

            if feature_engineering.cross:
                x_tt = calc_cross_feature(x_tt)

            y_pred: np.ndarray = 0.
            for model in models:
                y_pred += model.predict(x_tt) / len(models)
            y_pred = y_pred > 0
            pred_df[target] = y_pred.astype(int)
        else:
            pred_df[target] = 0
        env.predict(pred_df)

    elapsed_time = time.time() - time_start
    test_len = 15219  # length of test data (for developing API)
    print('End predicting')
    print(f'Prediction time: {elapsed_time} s, Prediction speed: {test_len / elapsed_time} iter/s')

    # move submission file generated by env into experiment directory
    shutil.move('submission.csv', f'{OUT_DIR}/submission.csv')

    return None


def get_model(model_name: str, model_param: DictConfig) -> Any:
    if model_name == 'XGBClassifier':
        if is_gpu():  # check if you're utilizing gpu if present
            assert model_param.tree_method == 'gpu_hist'
        return xgb.XGBClassifier(**model_param)
    elif model_name == 'LGBMClassifier':
        return lgb.LGBMClassifier(**model_param)
    elif model_name == 'CatBoostClassifier':
        return catboost.CatBoostClassifier(**model_param)
    else:
        raise ValueError(f'Invalid model_name: {model_name}')


def train_full(
        train: pd.DataFrame,
        features: List[str],
        target: str,
        model_name: str,
        model_param: DictConfig,
        train_param: DictConfig,
        OUT_DIR: str
        ) -> None:

    print('Start training')

    X_train = train.loc[:, features].values
    y_train = train.loc[:, target].values
    model = get_model(model_name, model_param)
    model.fit(X_train, y_train, **train_param)

    file = f'{OUT_DIR}/model_0.pkl'
    pickle.dump(model, open(file, 'wb'))
    mlflow.log_artifact(file)
    print('End training')

    return None


def train_cv(
        train: pd.DataFrame,
        features: List[str],
        target: str,
        model_name: str,
        model_param: DictConfig,
        train_param: DictConfig,
        cv_param: DictConfig,
        OUT_DIR: str
        ) -> None:

    kf = PurgedGroupTimeSeriesSplit(**cv_param)
    scores = []
    for fold, (tr, te) in enumerate(kf.split(train[target].values, train[target].values, train['date'].values)):
        print(f'Starting fold: {fold}, train size: {len(tr)}, validation size: {len(te)}')
        X_tr, X_val = train.loc[tr, features].values, train.loc[te, features].values
        y_tr, y_val = train.loc[tr, target].values, train.loc[te, target].values
        model = get_model(model_name, model_param)
        model.fit(X_tr, y_tr,
                  eval_set=[(X_tr, y_tr), (X_val, y_val)],
                  **train_param)

        pred_tr, pred_val = model.predict(X_tr), model.predict(X_val)
        auc = roc_auc_score(y_val, pred_val)
        utility_tr = utility_score_pd_scaled(
                        train.loc[tr, 'date'].values,
                        train.loc[tr, 'weight'].values,
                        train.loc[tr, 'resp'].values,
                        pred_tr)
        utility_val = utility_score_pd_scaled(
                        train.loc[te, 'date'].values,
                        train.loc[te, 'weight'].values,
                        train.loc[te, 'resp'].values,
                        pred_val)

        score = {'fold': fold, 'auc': auc, 'utility_tr': utility_tr, 'utility_val': utility_val}

        mlflow.log_metrics(score)
        scores.append(score)
        pprint.pprint(score)

        file = f'{OUT_DIR}/model_{fold}.pkl'
        pickle.dump(model, open(file, 'wb'))
        mlflow.log_artifact(file)
        del model, X_tr, X_val, y_tr, y_val

    return None


@hydra.main(config_path="./config", config_name="config")
def main(cfg: DictConfig) -> None:
    pprint.pprint(dict(cfg))
    DATA_DIR = get_datadir()
    OUT_DIR = f'{DATA_DIR}/{cfg.out_dir}'
    Path(OUT_DIR).mkdir(exist_ok=True, parents=True)

    # follow these sequences: uri > experiment > run > others
    tracking_uri = f'{DATA_DIR}/mlruns'
    mlflow.set_tracking_uri(tracking_uri)  # uri must be set before set_experiment
    mlflow.set_experiment(cfg.mlflow.experiment.name)
    mlflow.start_run()
    mlflow.set_tags(cfg.mlflow.experiment.tags)
    if not is_ipykernel():
        mlflow.log_artifacts('.hydra/')

    mlflow.log_param('feature_engineering', cfg.feature_engineering)
    mlflow.log_param('model.name', cfg.model.name)
    mlflow.log_params(cfg.model.model_param)
    mlflow.log_params(cfg.model.train_param)
    mlflow.log_param('cv.name', cfg.cv.name)
    mlflow.log_params(cfg.cv.param)
    mlflow.log_param('feature', cfg.features)

    # FE
    train = pd.DataFrame()

    # load feature, info
    features = []
    for f in cfg.features:
        df = pd.read_pickle(f'{DATA_DIR}/{f.path}').loc[:, f.cols]
        train = pd.concat([train, df], axis=1)
        features += f.cols
        print(f'Feature: {f.name}, shape: {df.shape}')

    # load info
    df = pd.read_pickle(f'{DATA_DIR}/{cfg.info.path}').loc[:, cfg.info.cols]
    train = pd.concat([train, df], axis=1)

    # load target
    df = pd.read_pickle(f'{DATA_DIR}/{cfg.target.path}').loc[:, cfg.target.col]
    train = pd.concat([train, df], axis=1)

    print(f'Input feature shape: {train.shape}')

    # Feature engineering
    if cfg.feature_engineering.weight_cutoff is not None:
        # cfg.weight.cutoff should be numerical
        train = train.query(f'weight > {cfg.feature_engineering.weight_cutoff}').reset_index(drop=True)

    # Fill missing values
    if cfg.feature_engineering.method_fillna == '-999':
        train.loc[:, features] = train.loc[:, features].fillna(-999)
    elif cfg.feature_engineering.method_fillna == 'forward':
        train.loc[:, features] = train.loc[:, features].fillna(method='ffill').fillna(0)
    elif cfg.feature_engineering.method_fillna is None:
        pass
    else:
        raise ValueError(f'Invalid method_fillna: {cfg.feature_engineering.method_fillna}')

    # Train
    if cfg.option.train:
        if cfg.cv.name == 'nocv':
            train_full(train, features, cfg.target.col, cfg.model.name, cfg.model.model_param, cfg.model.train_param, OUT_DIR)
        elif cfg.cv.name == 'PurgedGroupTimeSeriesSplit':
            train_cv(train, features, cfg.target.col, cfg.model.name, cfg.model.model_param, cfg.model.train_param, cfg.cv.param, OUT_DIR)
        else:
            raise ValueError(f'Invalid cv: {cfg.cv.name}')

    # Predict
    if cfg.option.predict:
        models = []
        for i in range(cfg.cv.param.n_splits):
            model = pd.read_pickle(open(f'{OUT_DIR}/model_{i}.pkl', 'rb'))
            models.append(model)

        predict(models, cfg.feature_engineering, cfg.target.col, OUT_DIR)

    return None


if __name__ == '__main__':
    main()
